#  快速入门


## 教程介绍

TKEStack 是一款面向私有化环境的开源容器编排引擎。在本教程中，您将了解

1. 如何创建 TKEStack 控制台和 global 集群，详见[安装步骤](../installation/installation-procedures.md)
2. 使用控制台创建和管理集群，详见[新建集群](#新建集群)
3. 在集群内快速、弹性地部署您的服务，详见[使用业务](#使用业务)

## 平台安装

参考：[安装步骤](../installation/installation-procedures.md)

## 新建集群

平台安装之后，可在【平台管理】控制台的【集群管理】中看到 global 集群。如下图所示：
   ![Global集群](../../../images/cluster.png)

TKEStack 还可以通过此处的蓝色按钮：**【新建独立集群】**和**【导入集群】**实现**多集群的管理**。

> 注意：**新建独立集群**和**导入已有集群**都属于[TKEStack架构](../installation/installation-architecture.md)中的**业务集群**。

### 新建独立集群

1. 登录 TKEStack，右上角会出现当前登录的用户名，示例为admin
2. 切换至【平台管理】控制台
3. 在“集群管理”页面中，单击【新建独立集群】，如下图所示：
   ![新建独立集群](../../../images/createCluster.png)
4. 在“新建独立集群”页面，填写集群的基本信息。新建的集群节点需满足[部署环境要求](../../../../docs/guide/zh-CN/installation/installation-requirement.md)，在满足需求之后，TKEStack的集群添加非常便利。如下图所示,只需填写【集群名称】、【目标机器】、【SSH端口】（默认22）、【密码】，其他保持默认即可添加新的集群。
   > 注意：若【保存】按钮是灰色，单击页面空白处即可变蓝

   ![集群基本信息0.png](../../../images/ClusterInfo.png)
   + **集群名称：** 支持**中文**，小于60字符即可

   + **Kubernetes版本：** 选择合适的kubernetes版本，各版本特性对比请查看 [Supported Versions of the Kubernetes Documentation](https://kubernetes.io/docs/home/supported-doc-versions/)（**建议使用默认值**）

   + **网卡名称：** 最长63个字符，只能包含小写字母、数字及分隔符(' - ')，且必须以小写字母开头，数字或小写字母结尾（**建议使用默认值eth0**）

   + **高可用类型** ：高可用 VIP 地址（**按需使用**）

     > 注意：如果使用高可用，至少需要三个 master 节点才可组成高可用集群，否则会出现 ***脑裂*** 现象。
   
     - **不设置**：第一台 master 节点的 IP 地址作为 APIServer 地址
     - **TKE 提供**：用户只需提供高可用的 IP 地址。TKE 部署 Keepalive，配置该 IP 为 Global 集群所有 Master 节点的VIP，以实现 Global 集群和控制台的高可用，此时该 VIP 和所有 Master 节点 IP 地址都是 APIServer 地址
     - **使用已有**：对接配置好的外部 LB 实例。VIP 绑定 Global 集群所有 Master 节点的 80（TKEStack 控制台）、443（TKEStack 控制台）、6443（kube-apiserver 端口）、31138（tke-auth-api 端口）端口，同时确保该 VIP 有至少两个 LB 后端（Master 节点），以避免 LB 单后端不可用风险

   + **GPU**：选择是否安装 GPU 相关依赖。（**按需使用**）
     
     > 注意：使用 GPU 首先确保节点有物理 GPU 卡，选择 GPU 类型后，平台将自动为节点安装相应的 GPU 驱动和运行时工具
     
     + **pGPU**：平台会自动为集群安装 [GPUManager](https://github.com/tkestack/docs/blob/master/features/gpumanager.md) 扩展组件，此时可以给负载分配任意整数张卡
     + **vGPU**：平台会自动为集群安装 [Nvidia-k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)，此时GPU可以被虚拟化，可以给负载分配非整数张GPU卡，例如可以给一个负载分配0.3个GPU
     
   + **容器网络** ：将为集群内容器分配在容器网络地址范围内的 IP 地址，您可以自定义三大私有网段作为容器网络， 根据您选择的集群内服务数量的上限，自动分配适当大小的 CIDR 段用于 kubernetes service；根据您选择 Pod 数量上限/节点，自动为集群内每台云服务器分配一个适当大小的网段用于该主机分配 Pod 的 IP 地址。（**建议使用默认值**）
     + **CIDR：** 集群内 Sevice、 Pod 等资源所在网段，注意：CIDR不能与目标机器IP段重叠， 否则会造成初始化失败
     + **Pod数量上限/节点：** 决定分配给每个 Node 的 CIDR 的大小
     + **Service数量上限/集群**：决定分配给 Sevice 的 CIDR 大小
     
   + **Master** ：输入目标机器信息后单击保存，**若保存按钮是灰色，单击网页空白处即可变蓝**
     
     > 注意：如果在之前选择了高可用，至少需要三个 master 节点才可组成高可用集群，否则会出现 ***脑裂*** 现象。
     
     + **目标机器**：Master 节点**内网 IP**，请配置**至少 8 Cores & 16G内存** 及以上的机型，**否则会部署失败**。注意：如上图所示，如果节点密码一样，这里可以通过英文的分号“;”分隔多个IP地址实现快速添加多个节点
     + **SSH端口**： 请确保目标机器安全组开放 22 端口和 ICMP 协议，否则无法远程登录和 PING 云服务器。（**建议使用默认值22**）
     + **主机label**：给主机设置Label,可用于指定容器调度。（**按需使用**）
     +  **认证方式**：连接目标机器的方式
        
        +  **密码认证**：
           +  **密码**：目标机器密码
        +  **密钥认证**：
           +  **私钥**：目标机器秘钥
           +  **私钥密码**：目标机器私钥密码，可选填
     + **GPU**： 使用GPU机器需提前安装驱动和runtime。（**按需使用**）
       
       > **添加机器**：可以通过节点下面的**【添加】**蓝色字体增加不同密码的master节点（**按需添加**）
5. **提交**： 集群信息填写完毕后，【提交】按钮变为可提交状态，单击即可提交。

### 导入已有集群

1. 登录 TKEStack。
2. 切换至【平台管理】控制台。
3. 在“集群管理”页面，单击【导入集群】。如下图所示：
   ![导入集群](../../../images/importCluster-1.png)
4. 在“导入集群”页面，填写被导入的集群信息。如下图所示：
   ![导入集群信息](../../../images/importCluster-2.png)
   - **名称**： 被导入集群的名称，最长60字符
   - **API Server**： 被导入集群的API server的域名或IP地址
   - **CertFile**： 输入被导入集群的 CertFile 文件内容
   - **Token**： 输入被导入集群创建时的token值
5. 单击最下方 【提交】 按钮 。

#### TKEStack 导入腾讯的 TKE 集群

1. 首先需要在TKE控制台所要导入的集群“基本信息”页里开启内/外网访问

   ![image-20200930154323734](../../../images/image-20200930154323734.png)

2. **APIServer 地址**：即上图中的访问地址，也可以根据上图中kubeconfig文件里的“server”字段内容填写。

3. **CertFile**：集群证书，kubeconfig中“certificate-authority-data”字段内容。

4. **Token**：由于目前TKE没有自动创建具有admin权限的token，这里需要手动创建，具体方式如下：

   1. 新建文件admin-role.yaml，用于生成kubernetes集群最高权限admin用户的token

      ```yaml
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: admin
        annotations:
          rbac.authorization.kubernetes.io/autoupdate: "true"
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: rbac.authorization.k8s.io
      subjects:
      - kind: ServiceAccount
        name: admin
        namespace: kube-system
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: admin
        namespace: kube-system
        labels:
          kubernetes.io/cluster-service: "true"
          addonmanager.kubernetes.io/mode: Reconcile
      ```

      

   2. 执行下面的命令创建 serviceaccount 和角色绑定

      ```shell
      kubectl create -f admin-role.yaml
      ```

   3. 创建完成后获取secret中token的值

      ```shell
      # 获取admin-token的secret名字
      $ kubectl -n kube-system get secret|grep admin-token
      admin-token-nwphb                          kubernetes.io/service-account-token   3         6m
      # 获取token的值
      $ kubectl -n kube-system describe secret admin-token-nwphb | grep token
      Name:         admin-token-w4wcd
      Type:  kubernetes.io/service-account-token
      token:            非常长的字符串
      ```

#### TKEStack 中导入 Rancher 的 RKE 集群

> 特别注意:注意RKE集群的 kubeconfig 中 clusters 字段里面的第一个 cluster 一般都是 Rancher 平台，而不是 RKE 集群。输入以下信息时，要确定选择正确的集群。

1. 获取 RKE 的 kubeconfig 文件

2. **APIServer 地址**：获取文件里面的“cluster”字段下“server”的内容。注意是引号里的全部内容

3. **CertFile**：集群证书，在上面的“server”地址的正下方，有集群证书字段“certificate-authority-data”。

   > 注意，Rancher 的 kubeconfig 这里的字段内容默认有“\”换行符，需要手动把内容里的换行符和空格全部去除。

4. **Token**：在“user”字段里面拥有用户的token

#### TKEStack 中导入阿里的 ACK 集群

1. 和TKE一样，需要获取开启外网访问的 ACK 的 kubeconfig 文件
2. **APIServer 地址**：获取文件里面的“cluster”字段下“server”的内容。
3. **CertFile**：集群证书，在上面的“server”地址正下方有集群证书字段“certificate-authority-data”。
4. **Token**：获取方式同 TKE，需要手动创建。

## 使用业务

TKEStack独创的概念，业务可以实现跨集群资源的分配与使用。

### 创建业务

1. 登录 TKEStack
2. 在【平台管理】控制台的【业务管理】中，单击 【新建业务】，如下图所示：
   ![新建业务](../../../images/createbusiness.png)
3. 在“新建业务”页面，填写业务信息，如下图所示：
   ![业务信息](../../../images/bussinessInfo.png)

   - **业务名称**：不能超过63个字符，这里以`my-business`为例
   - **业务成员**：  【访问管理】中[【用户管理】](../products/platform/accessmanagement/user.md)中的用户，这里以`admin`例，即这该用户可以访问这个业务。
   - **集群**：
     - 【集群管理】中的集群，这里以`gobal`集群为例
     - 【填写资源限制】可以设置当前业务使用该集群的资源上限（可不限制）
     - 【新增集群】可以添加多个集群，此业务可以使用多个集群的资源（按需添加）
   - **上级业务**：支持多级业务管理，按需选择（可不选）子业务继承父业务的所有资源。如果没有上级业务，业务可以选择所有集群的所有资源
4. 单击最下方 【完成】 按钮即可创建业务

### 创建业务下的命名空间
1. 登录 TKEStack

2. 在【平台管理】控制台的【业务管理】中，单击【业务id】，如下图所示：
   ![业务id](../../../images/businessid.png)
   
3. 单击【Namespace列表】，如下图标签1所示：
   > 该页面可以更改业务名称、成员、以及业务下集群资源的限制。
   
   ![命名空间列表](../../../images/businessns.png)
   
4. 单击【新建Namespace】，如下图所示：
   ![新建空间列表](../../../images/newns.png)

5. 在“新建Namespace”页面中，填写相关信息，如下图所示：
   ![新建空间列表](../../../images/my-ns.png)
   - **名称**：不能超过63个字符，这里以`new-ns`为例
   - **集群**：`my-business`业务中的集群，这里以`global`集群为例
   - **资源限制**：这里可以限制当前命名空间下各种资源的使用量，可以不设置


### 创建业务下的Deployment

1. 登录TKEStack，点击【平台管理】选项旁边的切换按钮，可以切换到【业务管理】控制台。

   > 注意：因为当前登录的是 admin 用户，【业务管理】控制台只包含在[创建的业务](#创建业务)中成员包含 admin 用户的业务，如果切换到【业务管理】控制台没有看见任何业务，请确认【平台管理】中【业务管理】中的相关业务的成员有没有当前用户，如没有，请添加当前用户。

2. 点击左侧导航栏中的【应用管理】，如果当前用户被分配了多个业务，可通过下图中标签3的选择框选择合适的业务。

3. 点击【工作负载】，点击下图标签4的【Deployment】，此时进入“Deployment”页面，可通过下图中的标签5选择Deployment的【命名空间】，如果没有命名空间，请先在【平台管理】下创建[业务的命名空间](#创建业务下的命名空间)：
   ![](../../../images/deployment.png)

4. 单击上图标签6【新建】，进入“新建 Workload ”页面。根据实际需求，设置 Deployment 参数。这里参数很多，其中必填信息已用红框标识：
   ![](../../../images/createDeployment-1.png)
    - **工作负载名**：输入自定义名称，这里以`my-dep`为例
   
    - **描述**：给工作负载添加描述，可不填
   
    - **标签**：给工作负载添加标签，通过工作负载名默认生成
   
    - **命名空间**：根据实际需求进行选择
   
    - **类型**：选择【Deployment（可扩展的部署 Pod）】
   ![](../../../images/createDeployment-2.png)
      
   - **数据卷（选填）**：为容器提供存储，目前支持临时路径、主机路径、云硬盘数据卷、文件存储NFS、配置文件、PVC，还需挂载到容器的指定路径中。如需指定容器挂载至指定路径时，单击【添加数据卷】
   
     * **临时目录**：主机上的一个临时目录，生命周期和Pod一致
     * **主机路径**：主机上的真实路径，可以重复使用，不会随Pod一起销毁
     * **NFS盘**：挂载外部 NFS 到Pod，用户需要指定相应NFS地址，格式：127.0.0.1:/data。请确保节点当中已经安装 nfs-utils包，才可正常使用nfs数据盘
     * **ConfigMap**：用户选择在业务Namespace下的 [ConfigMap](../products/business-control-pannel/application/configurations/ConfigMap.md)
     * **Secret**：用户选择在业务namespace下的 [Secret](../products/business-control-pannel/application/configurations/secret.md)
     * **PVC**：用户选择在业务namespace下的 [PVC](../products/business-control-pannel/application/storage/persistent-volume-claim.md)
     * **数据卷的名称**：给数据卷一个名称，以方便容器使用

   - **实例内容器**：根据实际需求，为 Deployment 的Pod 设置一个或多个不同的容器，如下图所示：
   
     ![](../../../images/createDeployment-3.png)
   
     * **名称**：自定义，这里以`my-container`为例
     
     * **镜像**：根据实际需求进行选择，这里以`nginx`为例
     
     * **镜像版本（Tag）**：根据实际需求进行填写，不填默认为`latest`
     
     * **CPU/内存限制**：可根据 [Kubernetes 资源限制](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) 进行设置 CPU 和内存的限制范围，提高业务的健壮性（**建议使用默认值**）
     
        * Request用于预分配资源,当集群中的节点没有request所要求的资源数量时,容器会创建失败。
        * Limit用于设置容器使用资源的最大上限,避免异常情况下节点资源消耗过多。
     
     * **GPU限制**：如容器内需要使用GPU，此处填GPU需求
     
     > 前提：节点有 GPU，并安装了 GPU 组件

     * **环境变量**：用于设置容器内的变量，变量名只能包含大小写字母、数字及下划线，并且不能以数字开头
        * **自定义**：自己设定变量键值对
        * **引用ConfigMap/Secret**：引用已有键值对
        * **Field**：自己设定变量名，变量值从负载的 YAML 文件中获取 metadata、spec、status数值
        * **ResourceFiled**：自己设定变量名，变量值从负载的 YAML 文件中获取 limit 和 request 数值
     
     * **高级设置**：可设置 “**工作目录**”、“**运行命令**”、“**运行参数**”、“**镜像更新策略**”、“**容器健康检查**”和“**特权级**”等参数。（按需使用）
        * **工作目录**：指定容器运行后的工作目录
     
        * **日志目录**：指定容器运行后的[日志目录](../products/business-control-pannel/operation/logcollect.md/#指定容器运行后的日志目录)
     
          > 1. 需要首先启用集群的 日志采集 功能
          > 2. 需要在创建爱你应用时挂载数据盘

        * **运行命令**：控制容器运行的输入命令，这里可以输入多条命令，注意每个命令单独一行
     
        * **运行参数**：传递给容器运行命令的输入参数，这里可以输入多条参数，注意每个参数单独一行
        
       * **镜像更新策略**：提供以下3种策略，请按需选择，若不设置镜像拉取策略，当镜像版本为空或 `latest` 时，使用 Always 策略，否则使用 IfNotPresent 策略
         
         * **Always**：总是从远程拉取该镜像
         * **IfNotPresent**：默认使用本地镜像，若本地无该镜像则远程拉取该镜像  
         * **Never**：只使用本地镜像，若本地没有该镜像将报异常
       
      * **容器健康检查**
     
        * **存活检查**：检查容器是否正常，不正常则重启实例。对于多活无状态的应用采用了存活探针 TCP 探测方式。存活探针组件包括 Gate、Keystone、Webshell、Nginx、Memcache 当连续探测容器端口不通，探针失败时，杀掉容器并重启。
        * **就绪检查**：检查容器是否就绪，不就绪则停止转发流量到当前实例。对于一主多备的服务采用就绪探针 TCP 探测方式，当探针失败时，将实例从 Service Endpoints 中移除。业务各个组件内部通过Kube-DNS访问CVM-Agent，就绪探针可以保证处于备机状态的 CVM 实例不存在于 Service Endpoints 中，并且将流量转发至主 CVM-Agent 上，从而保证服务的高可用。
     
       * **特权级容器**：容器开启特权级，将拥有宿主机的root权限
         
       * **权限集-增加**：增加权限集
         
       * **权限集-删除**：减少权限集
   
   * **实例数量**：根据实际需求选择调节方式，设置实例数量。
   ![](../../../images/createDeployment-4.png)

     - **手动调节**：直接设定实例个数
     - **自动调节**：依赖 [HPA](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/)，根据设定的触发条件自动调节实例个数，目前支持根据CPU、内存利用率等调节实例个数
     * **定时调节**：根据 [Crontab](https://baike.baidu.com/item/crontab) 语法周期性设置实例个数。前提：在[【扩展组件】](../products/platform/extender.md)里安装CronHPA
   * **显示高级设置**

   * **imagePullSecrets**：镜像拉取密钥，用于拉取用户的私有镜像，使用私有镜像首先需要新建Secret。如果是公有镜像，即支持匿名拉取，则可以忽略此步骤。
   - **节点调度策略**：根据配置的调度规则，将Pod调度到预期的节点。
     - **不使用调度策略**：k8s自动调度
     - **指定节点调度**：Pod 只调度到指定节点
     - **自定义调度规则**：通过节点的 Label 来实现
        - **强制满足条件**：调度期间如果满足亲和性条件则调度到对应node，如果没有节点满足条件则调度失败
        - **尽量满足条件**：调度期间如果满足亲和性条件则调度到对应node，如果没有节点满足条件则随机调度到任意节点
   * **注释（Annotations）**：给 deployment 添加相应 Annotation，如用户信息等
     
   * **网络模式**：选择Pod网络模式
      * **OverLay（虚拟网络）**：基于 IPIP 和 Host Gateway 的 Overlay 网络方案，每个实例拥有一个虚拟IP，集群外无法直接访问该IP
      * **FloatingIP（浮动 IP）**：为每个实例分配物理 IP，外部可直接访问。支持容器、物理机和虚拟机在同一个扁平面中直接通过IP进行通信的 Underlay 网络方案。提供了 IP 漂移能力，**支持 Pod 重启或迁移时 IP 不变**，跨机器迁移，实例ip也不会发生变化
      * **NAT（端口映射）**：Kubernetes 原生 NAT 网络方案，实例的端口映射到物理机的某个端口，但 IP 还是虚拟 IP ，可通过宿主机 IP 和映射端口访问，即 Pod 的容器中指定了 [hostPorts](https://kubernetes.io/zh/docs/concepts/policy/pod-security-policy/#host-namespaces)
      * **Host（主机网络）**：Kubernetes 原生 Host 网络方案，可以直接采用宿主机 IP 和端口，即 Pod 的 [hostNetwork=true](https://kubernetes.io/zh/docs/concepts/policy/pod-security-policy/#host-namespaces)

5. **创建Service（可选）**：

   ![](../../../images/createDeployment-5.png)

   - **Service**：勾选【启用】按钮，配置负载端口访问，将会创建于负载同名的 Service
   > 注意：如果不勾选【启用】则不会创建 Service

   - **服务访问方式**：选择是【仅在集群内部访问】该负载还是集群外部通过【主机端口访问】该负载
     
     - **仅在集群内访问**：使用 Service 的 ClusterIP 模式，自动分配 Service 网段中的 IP，用于集群内访问。数据库类等服务如 MySQL 可以选择集群内访问，以保证服务网络隔离
     - **主机端口访问**：提供一个主机端口映射到容器的访问方式，支持 TCP、UDP、Ingress。可用于业务定制上层 LB 转发到 Node
     - **Headless Service**：不创建用于集群内访问的 ClusterIP，访问 Service 名称时返回后端 `Pods IP:port`，用于适配自有的服务发现机制
     
   - **端口映射**：输入负载要暴露的端口并指定通信协议类型（**容器和服务端口建议都使用80**）

   - **Session Affinity:** 点击【显示高级设置】出现，表示会话保持。Service 有负载均衡的作用，有两种模式：RoundRobin 和 SessionAffinity（默认 None，按需使用）

     - ClientIP：基于客户端 IP 地址进行会话保持的模式， 即第1次将某个客户端发起的请求转发到后端的某个 Pod 上，之后从相同的客户端发起的请求都将被转发到后端相同的 Pod 上。即 Service 启用了 Session Affinity 负载分发策略。
     - Node：此时 Service 使用默认的 RoundRobin（轮询模式）进行负载分发，即轮询将请求转发到后端的各个 Pod 上。

6. 单击【创建Workload】，完成创建，如下图所示：

   ![](../../../images/workLoad.png)
   当“运行/期望Pod数量”相等时，即表示 Deployment 下的所有 Pod 已创建完成。

7. 如果在第5步中有创建Service，则可以在【服务】下的【Service】看到与刚刚创建的 Deployment 同名的 Service

   ![](../../../images/business-svc.png)

### 删除业务下的资源

在本节中，启动了`my-business`业务下的 Deployment 和 Service 两种资源，此步骤介绍如何清除所有资源。

#### 删除Deployment

1. 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】

2. 展开【工作负载】下拉项，进入 “Deployment” 管理页面，选择需要删除【Deployment】的业务下相应的【命名空间】，点击要删除的Deployment最右边的【更多】，点击【删除】。如下图所示：
   ![](../../../images/deletedeployment.png)
3. 在弹出框中单击【确定】，即可删除Deployment。

#### 删除Service

1. 登录 TKEStack，切换到【业务管理】控制台，选择左侧导航栏中的【应用管理】

2. 展开【服务】下拉项，进入 “Service” 管理页面，选择需要删除【Service】的业务下相应的【命名空间】，点击要删除的Service最右边的【删除】。如下图所示：
   ![](../../../images/deleteservice.png)
3. 在弹出框中单击【确定】，即可删除Service。
